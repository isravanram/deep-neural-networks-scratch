# üß† Neural Network from Scratch

This project demonstrates how to implement a **feedforward neural network** from scratch using only **NumPy**, without any high-level machine learning libraries like TensorFlow or PyTorch.


## üîç Overview

The implementation includes:
- Forward propagation
- Activation functions: Sigmoid, ReLU, Tanh, Softmax
- Cost functions: Binary cross-entropy & categorical cross-entropy
- Backpropagation (gradient descent)
- Weight & bias initialization
- Prediction and evaluation functions
- Support for **binary** and **multi-class classification**


This repository demonstrates how to implement a fully functional **Neural Network from scratch** using only **core Python and NumPy** ‚Äî without relying on any high-level ML libraries like TensorFlow, PyTorch, or Scikit-learn.

Designed as a hands-on deep dive, this project showcases:
- Manual forward and backward propagation
- Activation functions (ReLU, Sigmoid, Tanh, Softmax)
- Gradient descent optimization
- Binary & multi-class classification from first principles

üîç Built to reinforce foundational understanding and demonstrate the ability to translate theory into efficient, low-level implementation ‚Äî a skill essential for designing custom architectures and debugging complex models in real-world AI systems.

---

This version signals:
- You're **not a beginner** just learning the basics
- You‚Äôre intentionally revisiting fundamentals to demonstrate mastery
- You can **design, implement, and explain AI systems** without libraries
